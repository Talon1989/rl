import random
import gym
import numpy as np
from tensorflow import keras
import tensorflow.compat.v1 as tf
import matplotlib.pyplot as plt
import os
tf.disable_v2_behavior()


env = gym.make('CartPole-v0')
state_size = env.observation_space.shape[0]
n_actions = env.action_space.n
gamma = 0.95

def discount_and_normalize_rewards(ep_rewards):
    discounted_rewards = np.zeros_like(ep_rewards)
    reward_to_go = 0.
    for i in reversed(range(len(ep_rewards))):
        reward_to_go = reward_to_go * gamma * ep_rewards[i]
        discounted_rewards[i] = reward_to_go
    # normalization
    discounted_rewards = discounted_rewards - np.mean(discounted_rewards)
    discounted_rewards = discounted_rewards / np.std(discounted_rewards)
    return discounted_rewards


state_ph = tf.placeholder(tf.float32, [None, state_size], name='state_ph')
action_ph = tf.placeholder(tf.int32, [None, n_actions], name='action_ph')
discounted_rewards_ph = tf.placeholder(tf.float32, [None, ], name='discounted_rewards')

layer_1 = tf.layers.dense(state_ph, units=32, activation=tf.nn.relu)
layer_2 = tf.layers.dense(layer_1, units=n_actions)

proba_distr = tf.nn.softmax(layer_2)

neg_log_policy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer_2, labels=action_ph)
loss = tf.reduce_mean(neg_log_policy * discounted_rewards_ph)

train = tf.train.AdamOptimizer(0.01).minimize(loss)









































































































































































































































































































































